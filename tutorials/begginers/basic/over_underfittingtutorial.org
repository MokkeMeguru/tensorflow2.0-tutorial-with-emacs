# -*- org-export-babel-evaluate: nil -*-
#+options: ':nil *:t -:t ::t <:t H:3 \n:t ^:t arch:headline author:t
#+options: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+options: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t                                                     
#+title: 過剰適合と学習不足(overfitting and underfitting)
#+date: <2019-08-24 土>                                                                                           
#+author: MokkeMeguru                                                                                             
#+email: meguru.mokke@gmail.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.2 (Org mode 9.1.9)
#+LATEX_CLASS: extarticle
# #+LATEX_CLASS_OPTIONS: [a4paper, dvipdfmx, twocolumn, 8pt]
#+LATEX_CLASS_OPTIONS: [a4paper, dvipdfmx]
#+LATEX_HEADER: \usepackage{amsmath, amssymb, bm}
#+LATEX_HEADER: \usepackage{graphics}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{indentfirst}
#+LATEX_HEADER: \usepackage{pxjahyper}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[backend=biber, bibencoding=utf8, style=authoryear]{biblatex}
#+LATEX_HEADER: \usepackage[left=25truemm, right=25truemm]{geometry}
#+LATEX_HEADER: \usepackage{ascmac}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \hypersetup{ colorlinks=true, citecolor=blue, linkcolor=red, urlcolor=orange}
#+LATEX_HEADER: \addbibresource{reference.bib}
#+DESCRIPTION:
#+KEYWORDS:
#+STARTUP: indent overview inlineimages
#+PROPERTY: header-args :eval never-export
* TensorFlow ライブラリのインポート
  #+NAME: 45d7839c-e0db-41f8-a659-b85d2f83a416
  #+BEGIN_SRC ein-python :session localhost :exports both :results raw drawer
    from __future__ import division, absolute_import
    from __future__ import print_function, unicode_literals
    from functools import reduce

    import tensorflow as tf
    # import tensorflow_hub as hub

    from tensorflow import keras
    from tensorflow.keras import layers
    
    import numpy as np
    import matplotlib.pyplot as plt

    import pandas as pd
    # from sklearn.model_selection import train_test_split
    import seaborn as sns

    def print_infos(infolist: list):
        for info in infolist:
            print(info)


    print_infos([
        '{:25}: {}'.format("tensorflow\'s version", tf.__version__),
    ])
  #+END_SRC

  #+RESULTS: 45d7839c-e0db-41f8-a659-b85d2f83a416
  :results:
  tensorflow's version     : 2.0.0-beta1
  :end:

* データセットのダウンロード
  ~NUM_WORDS~ は語彙数。 ~keras.datasets.imdb.load_data~ によって頻度順な ~NUM_WORDS~ 個の単語によって辞書が形成され、それをもとに文章を数字列へ変換したデータセットが得られる。

  たとえば *a* などは高頻度な語彙であるから、数値化された際に、 *10* などが割り振られ、 *hogehogefoo* のような低頻度な語彙は、は語彙外の語として処理される。

  #+NAME: 45937806-8fc5-42cf-bd37-eba19a6c87c2
  #+BEGIN_SRC ein-python :session localhost :results raw drawer :exports both
    NUM_WORDS = 10000
    (train_data, train_labels), (test_data,
                                 test_labels) = keras.datasets.imdb.load_data(
                                     num_words=NUM_WORDS)


    def multi_hot_sequence(sequences, dimension):
        # create an all-zero matrix of shape [len(sequences), dimentions]
        results = np.zeros((len(sequences), dimension))
        for i, word_indices in enumerate(sequences):
            # set specific indices of results[i] to 1s
            results[i, word_indices] = 1.0
        return results


    train_data = multi_hot_sequence(train_data, dimension=NUM_WORDS)
    test_data = multi_hot_sequence(test_data, dimension=NUM_WORDS)
    plt.plot(train_data[0])
  #+END_SRC

  #+RESULTS: 45937806-8fc5-42cf-bd37-eba19a6c87c2
  :results:
  [<matplotlib.lines.Line2D at 0x7fc751a35cd0>]
  [[file:ein-images/ob-ein-0547b48bab64fe49872768adda6b3c64.png]]
  :end:

* 過剰適合のデモ
** ベースラインモデルの作成
   #+NAME: b3eeda7b-fdde-44c2-a184-bed8f09453a4
   #+BEGIN_SRC ein-python :session localhost :results raw drawer :exports both
     baseline_model = keras.Sequential([
         # `input_shape` is only required here so that `.summary` works.
         layers.Dense(16, activation='relu', input_shape=(NUM_WORDS, )),
         layers.Dense(16, activation='relu'),
         layers.Dense(1, activation='sigmoid')
     ])

     baseline_model.compile(optimizer='adam',
                            loss='binary_crossentropy',
                            metrics=['accuracy', 'binary_crossentropy'])

     baseline_model.summary()
   #+END_SRC

   #+RESULTS: b3eeda7b-fdde-44c2-a184-bed8f09453a4
   :results:
   Model: "sequential"
   _________________________________________________________________
   Layer (type)                 Output Shape              Param #   
   =================================================================
   dense_2 (Dense)              (None, 16)                160016    
   _________________________________________________________________
   dense_3 (Dense)              (None, 16)                272       
   _________________________________________________________________
   dense_4 (Dense)              (None, 1)                 17        
   =================================================================
   Total params: 160,305
   Trainable params: 160,305
   Non-trainable params: 0
   _________________________________________________________________
   :end:

   #+NAME: 6fd49bc0-005d-4095-a420-ff5ea72e058d
   #+BEGIN_SRC ein-python :session localhost :results raw drawer
     baseline_history = baseline_model.fit(train_data,
                                           train_labels,
                                           epochs=20,
                                           batch_size=512,
                                           validation_data=(test_data, test_labels),
                                           verbose=2
     )
   #+END_SRC

   #+RESULTS: 6fd49bc0-005d-4095-a420-ff5ea72e058d
   :results:
   WARNING: Logging before flag parsing goes to stderr.
   W0828 19:39:31.731766 140496544106112 deprecation.py:323] From /home/meguru/Github/tensorflow-2.0tutorial-with-emacs/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
   Instructions for updating:
   Use tf.where in 2.0, which has the same broadcast rule as np.where

   Train on 25000 samples, validate on 25000 samples
   Epoch 1/20

   25000/25000 - 3s - loss: 0.5142 - accuracy: 0.8078 - binary_crossentropy: 0.5142 - val_loss: 0.3610 - val_accuracy: 0.8706 - val_binary_crossentropy: 0.3610

   Epoch 2/20

   25000/25000 - 2s - loss: 0.2668 - accuracy: 0.9082 - binary_crossentropy: 0.2668 - val_loss: 0.2883 - val_accuracy: 0.8871 - val_binary_crossentropy: 0.2883

   Epoch 3/20

   25000/25000 - 3s - loss: 0.1926 - accuracy: 0.9333 - binary_crossentropy: 0.1926 - val_loss: 0.2882 - val_accuracy: 0.8846 - val_binary_crossentropy: 0.2882

   Epoch 4/20

   25000/25000 - 3s - loss: 0.1557 - accuracy: 0.9457 - binary_crossentropy: 0.1557 - val_loss: 0.3119 - val_accuracy: 0.8780 - val_binary_crossentropy: 0.3119

   Epoch 5/20

   25000/25000 - 2s - loss: 0.1285 - accuracy: 0.9572 - binary_crossentropy: 0.1285 - val_loss: 0.3277 - val_accuracy: 0.8756 - val_binary_crossentropy: 0.3277

   Epoch 6/20

   25000/25000 - 2s - loss: 0.1078 - accuracy: 0.9664 - binary_crossentropy: 0.1078 - val_loss: 0.3575 - val_accuracy: 0.8729 - val_binary_crossentropy: 0.3575

   Epoch 7/20

   25000/25000 - 2s - loss: 0.0907 - accuracy: 0.9720 - binary_crossentropy: 0.0907 - val_loss: 0.3919 - val_accuracy: 0.8686 - val_binary_crossentropy: 0.3919

   Epoch 8/20

   25000/25000 - 2s - loss: 0.0776 - accuracy: 0.9782 - binary_crossentropy: 0.0776 - val_loss: 0.4305 - val_accuracy: 0.8644 - val_binary_crossentropy: 0.4305

   Epoch 9/20

   25000/25000 - 2s - loss: 0.0636 - accuracy: 0.9834 - binary_crossentropy: 0.0636 - val_loss: 0.4681 - val_accuracy: 0.8623 - val_binary_crossentropy: 0.4681

   Epoch 10/20

   25000/25000 - 2s - loss: 0.0547 - accuracy: 0.9867 - binary_crossentropy: 0.0547 - val_loss: 0.5100 - val_accuracy: 0.8598 - val_binary_crossentropy: 0.5100

   Epoch 11/20

   25000/25000 - 2s - loss: 0.0446 - accuracy: 0.9908 - binary_crossentropy: 0.0446 - val_loss: 0.5541 - val_accuracy: 0.8560 - val_binary_crossentropy: 0.5541

   Epoch 12/20

   25000/25000 - 2s - loss: 0.0363 - accuracy: 0.9931 - binary_crossentropy: 0.0363 - val_loss: 0.5955 - val_accuracy: 0.8556 - val_binary_crossentropy: 0.5955

   Epoch 13/20

   25000/25000 - 2s - loss: 0.0304 - accuracy: 0.9950 - binary_crossentropy: 0.0304 - val_loss: 0.6521 - val_accuracy: 0.8514 - val_binary_crossentropy: 0.6521

   Epoch 14/20

   25000/25000 - 2s - loss: 0.0242 - accuracy: 0.9968 - binary_crossentropy: 0.0242 - val_loss: 0.6888 - val_accuracy: 0.8506 - val_binary_crossentropy: 0.6888

   Epoch 15/20

   25000/25000 - 2s - loss: 0.0195 - accuracy: 0.9978 - binary_crossentropy: 0.0195 - val_loss: 0.7171 - val_accuracy: 0.8496 - val_binary_crossentropy: 0.7171

   Epoch 16/20

   25000/25000 - 2s - loss: 0.0157 - accuracy: 0.9988 - binary_crossentropy: 0.0157 - val_loss: 0.7599 - val_accuracy: 0.8491 - val_binary_crossentropy: 0.7599

   Epoch 17/20

   25000/25000 - 3s - loss: 0.0131 - accuracy: 0.9991 - binary_crossentropy: 0.0131 - val_loss: 0.7925 - val_accuracy: 0.8484 - val_binary_crossentropy: 0.7925

   Epoch 18/20

   25000/25000 - 3s - loss: 0.0108 - accuracy: 0.9994 - binary_crossentropy: 0.0108 - val_loss: 0.8273 - val_accuracy: 0.8482 - val_binary_crossentropy: 0.8273

   Epoch 19/20

   25000/25000 - 2s - loss: 0.0090 - accuracy: 0.9995 - binary_crossentropy: 0.0090 - val_loss: 0.8625 - val_accuracy: 0.8476 - val_binary_crossentropy: 0.8625

   Epoch 20/20

   25000/25000 - 2s - loss: 0.0076 - accuracy: 0.9995 - binary_crossentropy: 0.0076 - val_loss: 0.8872 - val_accuracy: 0.8468 - val_binary_crossentropy: 0.8872
   :end:
   
      Epoch 20/20

      25000/25000 - 2s - loss: 0.0076 - accuracy: 0.9995 - binary_crossentropy: 0.0076 - val_loss: 0.8872 - val_accuracy: 0.8468 - val_binary_crossentropy: 0.8872

** より小さめのモデルの作成
   #+NAME: 5e7d009a-dc51-4afc-8120-b831dc84a3d9
   #+BEGIN_SRC ein-python :session localhost :results raw drawer :exports both
     smaller_model = keras.Sequential([
         layers.Dense(4, activation='relu', input_shape=(NUM_WORDS,)),
         layers.Dense(4, activation='relu'),
         layers.Dense(1, activation='sigmoid')
     ])

     smaller_model.compile(optimizer='adam',
                           loss='binary_crossentropy',
                           metrics=['accuracy', 'binary_crossentropy']
     )

     smaller_model.summary()
   #+END_SRC

   #+RESULTS: 5e7d009a-dc51-4afc-8120-b831dc84a3d9
   :results:
   Model: "sequential_2"
   _________________________________________________________________
   Layer (type)                 Output Shape              Param #   
   =================================================================
   dense_8 (Dense)              (None, 4)                 40004     
   _________________________________________________________________
   dense_9 (Dense)              (None, 4)                 20        
   _________________________________________________________________
   dense_10 (Dense)             (None, 1)                 5         
   =================================================================
   Total params: 40,029
   Trainable params: 40,029
   Non-trainable params: 0
   _________________________________________________________________
   :end:

   #+NAME: bf9d7da6-6b84-4462-b109-91fc0bffa49c
   #+BEGIN_SRC ein-python :session localhost :results raw drawer
     smaller_history = smaller_model.fit(train_data,
                                         train_labels,
                                         epochs=20,
                                         batch_size=512,
                                         validation_data=(test_data, test_labels),
                                         verbose=2)
   #+END_SRC

   #+RESULTS: bf9d7da6-6b84-4462-b109-91fc0bffa49c
   :results:
   Train on 25000 samples, validate on 25000 samples
   Epoch 1/20

   25000/25000 - 3s - loss: 0.5896 - accuracy: 0.7092 - binary_crossentropy: 0.5896 - val_loss: 0.4689 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.4689

   Epoch 2/20

   25000/25000 - 2s - loss: 0.3566 - accuracy: 0.8904 - binary_crossentropy: 0.3566 - val_loss: 0.3324 - val_accuracy: 0.8831 - val_binary_crossentropy: 0.3324

   Epoch 3/20

   25000/25000 - 2s - loss: 0.2518 - accuracy: 0.9206 - binary_crossentropy: 0.2518 - val_loss: 0.2950 - val_accuracy: 0.8880 - val_binary_crossentropy: 0.2950

   Epoch 4/20

   25000/25000 - 2s - loss: 0.2041 - accuracy: 0.9346 - binary_crossentropy: 0.2041 - val_loss: 0.2854 - val_accuracy: 0.8880 - val_binary_crossentropy: 0.2854

   Epoch 5/20

   25000/25000 - 2s - loss: 0.1752 - accuracy: 0.9440 - binary_crossentropy: 0.1752 - val_loss: 0.2873 - val_accuracy: 0.8861 - val_binary_crossentropy: 0.2873

   Epoch 6/20

   25000/25000 - 3s - loss: 0.1542 - accuracy: 0.9513 - binary_crossentropy: 0.1542 - val_loss: 0.2957 - val_accuracy: 0.8828 - val_binary_crossentropy: 0.2957

   Epoch 7/20

   25000/25000 - 2s - loss: 0.1366 - accuracy: 0.9577 - binary_crossentropy: 0.1366 - val_loss: 0.3027 - val_accuracy: 0.8816 - val_binary_crossentropy: 0.3027

   Epoch 8/20

   25000/25000 - 2s - loss: 0.1226 - accuracy: 0.9627 - binary_crossentropy: 0.1226 - val_loss: 0.3148 - val_accuracy: 0.8783 - val_binary_crossentropy: 0.3148

   Epoch 9/20

   25000/25000 - 2s - loss: 0.1101 - accuracy: 0.9676 - binary_crossentropy: 0.1101 - val_loss: 0.3331 - val_accuracy: 0.8747 - val_binary_crossentropy: 0.3331

   Epoch 10/20

   25000/25000 - 3s - loss: 0.0999 - accuracy: 0.9721 - binary_crossentropy: 0.0999 - val_loss: 0.3431 - val_accuracy: 0.8742 - val_binary_crossentropy: 0.3431

   Epoch 11/20

   25000/25000 - 2s - loss: 0.0905 - accuracy: 0.9752 - binary_crossentropy: 0.0905 - val_loss: 0.3613 - val_accuracy: 0.8713 - val_binary_crossentropy: 0.3613

   Epoch 12/20

   25000/25000 - 2s - loss: 0.0822 - accuracy: 0.9788 - binary_crossentropy: 0.0822 - val_loss: 0.3786 - val_accuracy: 0.8697 - val_binary_crossentropy: 0.3786

   Epoch 13/20

   25000/25000 - 3s - loss: 0.0743 - accuracy: 0.9820 - binary_crossentropy: 0.0743 - val_loss: 0.3984 - val_accuracy: 0.8671 - val_binary_crossentropy: 0.3984

   Epoch 14/20

   25000/25000 - 2s - loss: 0.0675 - accuracy: 0.9844 - binary_crossentropy: 0.0675 - val_loss: 0.4155 - val_accuracy: 0.8665 - val_binary_crossentropy: 0.4155

   Epoch 15/20

   25000/25000 - 2s - loss: 0.0616 - accuracy: 0.9870 - binary_crossentropy: 0.0616 - val_loss: 0.4384 - val_accuracy: 0.8632 - val_binary_crossentropy: 0.4384

   Epoch 16/20

   25000/25000 - 2s - loss: 0.0556 - accuracy: 0.9886 - binary_crossentropy: 0.0556 - val_loss: 0.4540 - val_accuracy: 0.8640 - val_binary_crossentropy: 0.4540

   Epoch 17/20

   25000/25000 - 2s - loss: 0.0499 - accuracy: 0.9909 - binary_crossentropy: 0.0499 - val_loss: 0.4755 - val_accuracy: 0.8618 - val_binary_crossentropy: 0.4755

   Epoch 18/20

   25000/25000 - 2s - loss: 0.0449 - accuracy: 0.9922 - binary_crossentropy: 0.0449 - val_loss: 0.4970 - val_accuracy: 0.8601 - val_binary_crossentropy: 0.4970

   Epoch 19/20

   25000/25000 - 2s - loss: 0.0406 - accuracy: 0.9936 - binary_crossentropy: 0.0406 - val_loss: 0.5163 - val_accuracy: 0.8602 - val_binary_crossentropy: 0.5163

   Epoch 20/20

   25000/25000 - 2s - loss: 0.0364 - accuracy: 0.9948 - binary_crossentropy: 0.0364 - val_loss: 0.5384 - val_accuracy: 0.8593 - val_binary_crossentropy: 0.5384
   :end:

   Epoch 20/20

   25000/25000 - 2s - loss: 0.0364 - accuracy: 0.9948 - binary_crossentropy: 0.0364 - val_loss: 0.5384 - val_accuracy: 0.8593 - val_binary_crossentropy: 0.5384
** より大きめのモデルの作成
   #+NAME: d8a7a8b8-2a3d-4648-9d8b-3c9f4ea110d2
   #+BEGIN_SRC ein-python :session localhost :results raw drawer :exports both
     bigger_model = keras.models.Sequential([
         layers.Dense(512, activation='relu', input_shape=(NUM_WORDS, )),
         layers.Dense(512, activation='relu'),
         layers.Dense(1, activation='sigmoid')
     ])

     bigger_model.compile(optimizer='adam',
                          loss='binary_crossentropy',
                          metrics=['accuracy', 'binary_crossentropy'])

     bigger_model.summary()
   #+END_SRC

   #+RESULTS: d8a7a8b8-2a3d-4648-9d8b-3c9f4ea110d2
   :results:
   Model: "sequential_4"
   _________________________________________________________________
   Layer (type)                 Output Shape              Param #   
   =================================================================
   dense_14 (Dense)             (None, 512)               5120512   
   _________________________________________________________________
   dense_15 (Dense)             (None, 512)               262656    
   _________________________________________________________________
   dense_16 (Dense)             (None, 1)                 513       
   =================================================================
   Total params: 5,383,681
   Trainable params: 5,383,681
   Non-trainable params: 0
   _________________________________________________________________
   :end:

   #+NAME: 0528cfda-bda3-4b76-8692-a11035e32f73
   #+BEGIN_SRC ein-python :session localhost :results raw drawer
     bigger_history = bigger_model.fit(train_data,
                                       train_labels,
                                       epochs=20,
                                       batch_size=512,
                                       validation_data=(test_data, test_labels),
                                       verbose=2)
   #+END_SRC

   #+RESULTS: 0528cfda-bda3-4b76-8692-a11035e32f73
   :results:
   Train on 25000 samples, validate on 25000 samples
   Epoch 1/20

   25000/25000 - 3s - loss: 0.3399 - accuracy: 0.8564 - binary_crossentropy: 0.3399 - val_loss: 0.2951 - val_accuracy: 0.8786 - val_binary_crossentropy: 0.2951

   Epoch 2/20

   25000/25000 - 2s - loss: 0.1411 - accuracy: 0.9479 - binary_crossentropy: 0.1411 - val_loss: 0.3331 - val_accuracy: 0.8725 - val_binary_crossentropy: 0.3331

   Epoch 3/20

   25000/25000 - 2s - loss: 0.0447 - accuracy: 0.9868 - binary_crossentropy: 0.0447 - val_loss: 0.4538 - val_accuracy: 0.8679 - val_binary_crossentropy: 0.4538

   Epoch 4/20

   25000/25000 - 2s - loss: 0.0067 - accuracy: 0.9988 - binary_crossentropy: 0.0067 - val_loss: 0.6032 - val_accuracy: 0.8670 - val_binary_crossentropy: 0.6032

   Epoch 5/20

   25000/25000 - 3s - loss: 0.0012 - accuracy: 0.9999 - binary_crossentropy: 0.0012 - val_loss: 0.6938 - val_accuracy: 0.8677 - val_binary_crossentropy: 0.6938

   Epoch 6/20

   25000/25000 - 3s - loss: 8.5360e-04 - accuracy: 1.0000 - binary_crossentropy: 8.5360e-04 - val_loss: 0.7123 - val_accuracy: 0.8702 - val_binary_crossentropy: 0.7123

   Epoch 7/20

   25000/25000 - 3s - loss: 1.3692e-04 - accuracy: 1.0000 - binary_crossentropy: 1.3692e-04 - val_loss: 0.7403 - val_accuracy: 0.8705 - val_binary_crossentropy: 0.7403

   Epoch 8/20

   25000/25000 - 3s - loss: 9.3140e-05 - accuracy: 1.0000 - binary_crossentropy: 9.3140e-05 - val_loss: 0.7625 - val_accuracy: 0.8704 - val_binary_crossentropy: 0.7625

   Epoch 9/20

   25000/25000 - 3s - loss: 7.0060e-05 - accuracy: 1.0000 - binary_crossentropy: 7.0060e-05 - val_loss: 0.7792 - val_accuracy: 0.8706 - val_binary_crossentropy: 0.7792

   Epoch 10/20

   25000/25000 - 3s - loss: 5.5370e-05 - accuracy: 1.0000 - binary_crossentropy: 5.5370e-05 - val_loss: 0.7929 - val_accuracy: 0.8709 - val_binary_crossentropy: 0.7929

   Epoch 11/20

   25000/25000 - 3s - loss: 4.4950e-05 - accuracy: 1.0000 - binary_crossentropy: 4.4950e-05 - val_loss: 0.8057 - val_accuracy: 0.8706 - val_binary_crossentropy: 0.8057

   Epoch 12/20

   25000/25000 - 3s - loss: 3.7448e-05 - accuracy: 1.0000 - binary_crossentropy: 3.7448e-05 - val_loss: 0.8164 - val_accuracy: 0.8704 - val_binary_crossentropy: 0.8164

   Epoch 13/20

   25000/25000 - 3s - loss: 3.1557e-05 - accuracy: 1.0000 - binary_crossentropy: 3.1557e-05 - val_loss: 0.8251 - val_accuracy: 0.8707 - val_binary_crossentropy: 0.8251

   Epoch 14/20

   25000/25000 - 3s - loss: 2.7019e-05 - accuracy: 1.0000 - binary_crossentropy: 2.7019e-05 - val_loss: 0.8345 - val_accuracy: 0.8704 - val_binary_crossentropy: 0.8345

   Epoch 15/20

   25000/25000 - 3s - loss: 2.3358e-05 - accuracy: 1.0000 - binary_crossentropy: 2.3358e-05 - val_loss: 0.8425 - val_accuracy: 0.8704 - val_binary_crossentropy: 0.8425

   Epoch 16/20

   25000/25000 - 3s - loss: 2.0423e-05 - accuracy: 1.0000 - binary_crossentropy: 2.0423e-05 - val_loss: 0.8504 - val_accuracy: 0.8705 - val_binary_crossentropy: 0.8504

   Epoch 17/20

   25000/25000 - 3s - loss: 1.7946e-05 - accuracy: 1.0000 - binary_crossentropy: 1.7946e-05 - val_loss: 0.8574 - val_accuracy: 0.8706 - val_binary_crossentropy: 0.8574

   Epoch 18/20

   25000/25000 - 2s - loss: 1.5874e-05 - accuracy: 1.0000 - binary_crossentropy: 1.5874e-05 - val_loss: 0.8641 - val_accuracy: 0.8706 - val_binary_crossentropy: 0.8641

   Epoch 19/20

   25000/25000 - 2s - loss: 1.4139e-05 - accuracy: 1.0000 - binary_crossentropy: 1.4139e-05 - val_loss: 0.8698 - val_accuracy: 0.8705 - val_binary_crossentropy: 0.8698

   Epoch 20/20

   25000/25000 - 3s - loss: 1.2666e-05 - accuracy: 1.0000 - binary_crossentropy: 1.2666e-05 - val_loss: 0.8758 - val_accuracy: 0.8706 - val_binary_crossentropy: 0.8758
   :end:
   
   Epoch 20/20

   25000/25000 - 3s - loss: 1.2666e-05 - accuracy: 1.0000 - binary_crossentropy: 1.2666e-05 - val_loss: 0.8758 - val_accuracy: 0.8706 - val_binary_crossentropy: 0.8758

** 訓練の比較
   Train は訓練時の loss、Val は検証時の loss を示している。
   #+NAME: 36de832d-d89e-43b8-b204-3dd4ebc65a9a
   #+BEGIN_SRC ein-python :session localhost :results raw drawer :exports both
     def plot_history(histories, key='binary_crossentropy'):
         plt.figure(figsize=(16, 10))
         for name, history in histories:
             val = plt.plot(history.epoch,
                            history.history['val_' + key],
                            '--',
                            label=name.title() + ' Val')
             plt.plot(history.epoch,
                      history.history[key],
                      color=val[0].get_color(),
                      label=name.title() + ' Train')
             plt.xlabel('Epochs')
             plt.ylabel(key.replace('-', ' ').title())
             plt.legend()

             plt.xlim([0, max(history.epoch)])


     plot_history([('baseline', baseline_history), ('smaller', smaller_history),
                   ('bigger', bigger_history)])
   #+END_SRC

   #+RESULTS: 36de832d-d89e-43b8-b204-3dd4ebc65a9a
   :results:
   [[file:ein-images/ob-ein-1a5ebbfa75d693c9825414506ee5bae4.png]]
   :end:

* 過剰適合を防ぐための戦略
** 重みの正則化
   - L1 正則化
     
     スパース性を導入することができる。一部の重みパラメータをゼロにすることができる。(不要な説明変数を省くことができる。)
   - L2 正則化
     
     重みパラメータをスパースにすることなくペナルティを課すことができる。(より滑らかに正則化をすることができる)
   
   過剰適合を防ぐ目的ならば、一般的にL2 正則化が推奨される。
   
   #+NAME: ba77b5d5-858f-42f0-8dbd-2ae111cd58ac
   #+BEGIN_SRC ein-python :session localhost :results raw drawer
     l2_model = keras.models.Sequential([
         layers.Dense(16,
                      kernel_regularizer=keras.regularizers.l2(0.001),
                      activation='relu',
                      input_shape=(NUM_WORDS, )),
         layers.Dense(16,
                      kernel_regularizer=keras.regularizers.l2(0.001),
                      activation='relu'),
         layers.Dense(1, activation='sigmoid')
     ])

     l2_model.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy', 'binary_crossentropy'])

     l2_model_history = l2_model.fit(train_data,
                                     train_labels,
                                     epochs=20,
                                     batch_size=512,
                                     validation_data=(test_data, test_labels),
                                     verbose=2)
   #+END_SRC

   #+RESULTS: ba77b5d5-858f-42f0-8dbd-2ae111cd58ac
   :results:
   Train on 25000 samples, validate on 25000 samples
   Epoch 1/20

   25000/25000 - 2s - loss: 0.5409 - accuracy: 0.8101 - binary_crossentropy: 0.5015 - val_loss: 0.3960 - val_accuracy: 0.8711 - val_binary_crossentropy: 0.3559

   Epoch 2/20

   25000/25000 - 2s - loss: 0.3162 - accuracy: 0.9038 - binary_crossentropy: 0.2721 - val_loss: 0.3363 - val_accuracy: 0.8870 - val_binary_crossentropy: 0.2889

   Epoch 3/20

   25000/25000 - 2s - loss: 0.2589 - accuracy: 0.9270 - binary_crossentropy: 0.2091 - val_loss: 0.3351 - val_accuracy: 0.8864 - val_binary_crossentropy: 0.2836

   Epoch 4/20

   25000/25000 - 2s - loss: 0.2339 - accuracy: 0.9372 - binary_crossentropy: 0.1806 - val_loss: 0.3453 - val_accuracy: 0.8832 - val_binary_crossentropy: 0.2909

   Epoch 5/20

   25000/25000 - 2s - loss: 0.2170 - accuracy: 0.9457 - binary_crossentropy: 0.1615 - val_loss: 0.3573 - val_accuracy: 0.8791 - val_binary_crossentropy: 0.3008

   Epoch 6/20

   25000/25000 - 2s - loss: 0.2045 - accuracy: 0.9507 - binary_crossentropy: 0.1470 - val_loss: 0.3685 - val_accuracy: 0.8782 - val_binary_crossentropy: 0.3106

   Epoch 7/20

   25000/25000 - 3s - loss: 0.1960 - accuracy: 0.9540 - binary_crossentropy: 0.1373 - val_loss: 0.3900 - val_accuracy: 0.8730 - val_binary_crossentropy: 0.3307

   Epoch 8/20

   25000/25000 - 2s - loss: 0.1879 - accuracy: 0.9574 - binary_crossentropy: 0.1279 - val_loss: 0.3985 - val_accuracy: 0.8724 - val_binary_crossentropy: 0.3382

   Epoch 9/20

   25000/25000 - 3s - loss: 0.1807 - accuracy: 0.9599 - binary_crossentropy: 0.1196 - val_loss: 0.4163 - val_accuracy: 0.8676 - val_binary_crossentropy: 0.3547

   Epoch 10/20

   25000/25000 - 2s - loss: 0.1752 - accuracy: 0.9617 - binary_crossentropy: 0.1132 - val_loss: 0.4287 - val_accuracy: 0.8684 - val_binary_crossentropy: 0.3663

   Epoch 11/20

   25000/25000 - 3s - loss: 0.1692 - accuracy: 0.9653 - binary_crossentropy: 0.1062 - val_loss: 0.4455 - val_accuracy: 0.8668 - val_binary_crossentropy: 0.3820

   Epoch 12/20

   25000/25000 - 3s - loss: 0.1647 - accuracy: 0.9648 - binary_crossentropy: 0.1007 - val_loss: 0.4617 - val_accuracy: 0.8635 - val_binary_crossentropy: 0.3973

   Epoch 13/20

   25000/25000 - 2s - loss: 0.1590 - accuracy: 0.9696 - binary_crossentropy: 0.0941 - val_loss: 0.4757 - val_accuracy: 0.8616 - val_binary_crossentropy: 0.4105

   Epoch 14/20

   25000/25000 - 2s - loss: 0.1539 - accuracy: 0.9712 - binary_crossentropy: 0.0883 - val_loss: 0.4928 - val_accuracy: 0.8605 - val_binary_crossentropy: 0.4269

   Epoch 15/20

   25000/25000 - 2s - loss: 0.1506 - accuracy: 0.9732 - binary_crossentropy: 0.0844 - val_loss: 0.5135 - val_accuracy: 0.8602 - val_binary_crossentropy: 0.4469

   Epoch 16/20

   25000/25000 - 2s - loss: 0.1478 - accuracy: 0.9744 - binary_crossentropy: 0.0804 - val_loss: 0.5235 - val_accuracy: 0.8568 - val_binary_crossentropy: 0.4557

   Epoch 17/20

   25000/25000 - 2s - loss: 0.1445 - accuracy: 0.9753 - binary_crossentropy: 0.0767 - val_loss: 0.5315 - val_accuracy: 0.8585 - val_binary_crossentropy: 0.4633

   Epoch 18/20

   25000/25000 - 2s - loss: 0.1428 - accuracy: 0.9759 - binary_crossentropy: 0.0739 - val_loss: 0.5648 - val_accuracy: 0.8558 - val_binary_crossentropy: 0.4953

   Epoch 19/20

   25000/25000 - 2s - loss: 0.1408 - accuracy: 0.9764 - binary_crossentropy: 0.0711 - val_loss: 0.5536 - val_accuracy: 0.8558 - val_binary_crossentropy: 0.4833

   Epoch 20/20

   25000/25000 - 2s - loss: 0.1314 - accuracy: 0.9818 - binary_crossentropy: 0.0613 - val_loss: 0.5645 - val_accuracy: 0.8550 - val_binary_crossentropy: 0.4947
   :end:
   
   Epoch 20/20

   25000/25000 - 2s - loss: 0.1314 - accuracy: 0.9818 - binary_crossentropy: 0.0613 - val_loss: 0.5645 - val_accuracy: 0.8550 - val_binary_crossentropy: 0.4947

   #+NAME: 909f4fd7-2e05-40aa-a46f-2c36265324e2
   #+BEGIN_SRC ein-python :session localhost :results raw drawer
     plot_history([('baseline', baseline_history), ('l2', l2_model_history)])
   #+END_SRC

   #+RESULTS: 909f4fd7-2e05-40aa-a46f-2c36265324e2
   :results:
   [[file:ein-images/ob-ein-137acd35541b6f26668a9ece7cfbcf83.png]]
   :end:
** Dropout の導入
   #+NAME: 8e120488-472c-4aa9-a9c6-6f07a4b60e4d
   #+BEGIN_SRC ein-python :session localhost :results raw drawer :exports both
     dpt_model = keras.models.Sequential([
         layers.Dense(16, activation='relu', input_shape=(NUM_WORDS, )),
         layers.Dropout(0.5),
         layers.Dense(16, activation='relu'),
         layers.Dropout(0.5),
         layers.Dense(1, activation='sigmoid')
     ])

     dpt_model.compile(optimizer='adam',
                       loss='binary_crossentropy',
                       metrics=['accuracy', 'binary_crossentropy'])

     dpt_model.summary()
   #+END_SRC

   #+RESULTS: 8e120488-472c-4aa9-a9c6-6f07a4b60e4d
   :results:
   Model: "sequential_6"
   _________________________________________________________________
   Layer (type)                 Output Shape              Param #   
   =================================================================
   dense_20 (Dense)             (None, 16)                160016    
   _________________________________________________________________
   dropout (Dropout)            (None, 16)                0         
   _________________________________________________________________
   dense_21 (Dense)             (None, 16)                272       
   _________________________________________________________________
   dropout_1 (Dropout)          (None, 16)                0         
   _________________________________________________________________
   dense_22 (Dense)             (None, 1)                 17        
   =================================================================
   Total params: 160,305
   Trainable params: 160,305
   Non-trainable params: 0
   _________________________________________________________________
   :end:


   
#+NAME: e3debc36-d0f5-4e76-b64b-52e413778ad6
#+BEGIN_SRC ein-python :session localhost :results raw drawer
  dpt_model_history = dpt_model.fit(train_data, train_labels,
                                    epochs=20,
                                    batch_size=512,
                                    validation_data=(test_data, test_labels),
                                    verbose=2)
#+END_SRC

#+RESULTS: e3debc36-d0f5-4e76-b64b-52e413778ad6
:results:
Train on 25000 samples, validate on 25000 samples
Epoch 1/20

25000/25000 - 3s - loss: 0.6396 - accuracy: 0.6274 - binary_crossentropy: 0.6396 - val_loss: 0.5325 - val_accuracy: 0.8416 - val_binary_crossentropy: 0.5325

Epoch 2/20

25000/25000 - 2s - loss: 0.4798 - accuracy: 0.7875 - binary_crossentropy: 0.4798 - val_loss: 0.3594 - val_accuracy: 0.8782 - val_binary_crossentropy: 0.3594

Epoch 3/20

25000/25000 - 2s - loss: 0.3708 - accuracy: 0.8568 - binary_crossentropy: 0.3708 - val_loss: 0.2963 - val_accuracy: 0.8881 - val_binary_crossentropy: 0.2963

Epoch 4/20

25000/25000 - 2s - loss: 0.2968 - accuracy: 0.8916 - binary_crossentropy: 0.2968 - val_loss: 0.2730 - val_accuracy: 0.8896 - val_binary_crossentropy: 0.2730

Epoch 5/20

25000/25000 - 2s - loss: 0.2541 - accuracy: 0.9154 - binary_crossentropy: 0.2541 - val_loss: 0.2731 - val_accuracy: 0.8906 - val_binary_crossentropy: 0.2731

Epoch 6/20

25000/25000 - 2s - loss: 0.2179 - accuracy: 0.9270 - binary_crossentropy: 0.2179 - val_loss: 0.2848 - val_accuracy: 0.8864 - val_binary_crossentropy: 0.2848

Epoch 7/20

25000/25000 - 2s - loss: 0.1919 - accuracy: 0.9362 - binary_crossentropy: 0.1919 - val_loss: 0.2994 - val_accuracy: 0.8862 - val_binary_crossentropy: 0.2994

Epoch 8/20

25000/25000 - 2s - loss: 0.1677 - accuracy: 0.9454 - binary_crossentropy: 0.1677 - val_loss: 0.3094 - val_accuracy: 0.8850 - val_binary_crossentropy: 0.3094

Epoch 9/20

25000/25000 - 2s - loss: 0.1528 - accuracy: 0.9506 - binary_crossentropy: 0.1528 - val_loss: 0.3390 - val_accuracy: 0.8805 - val_binary_crossentropy: 0.3390

Epoch 10/20

25000/25000 - 2s - loss: 0.1369 - accuracy: 0.9550 - binary_crossentropy: 0.1369 - val_loss: 0.3557 - val_accuracy: 0.8800 - val_binary_crossentropy: 0.3557

Epoch 11/20

25000/25000 - 2s - loss: 0.1244 - accuracy: 0.9590 - binary_crossentropy: 0.1244 - val_loss: 0.3769 - val_accuracy: 0.8815 - val_binary_crossentropy: 0.3769

Epoch 12/20

25000/25000 - 2s - loss: 0.1115 - accuracy: 0.9631 - binary_crossentropy: 0.1115 - val_loss: 0.3882 - val_accuracy: 0.8778 - val_binary_crossentropy: 0.3882

Epoch 13/20

25000/25000 - 2s - loss: 0.1039 - accuracy: 0.9659 - binary_crossentropy: 0.1039 - val_loss: 0.4071 - val_accuracy: 0.8776 - val_binary_crossentropy: 0.4071

Epoch 14/20

25000/25000 - 2s - loss: 0.0949 - accuracy: 0.9679 - binary_crossentropy: 0.0949 - val_loss: 0.4291 - val_accuracy: 0.8775 - val_binary_crossentropy: 0.4291

Epoch 15/20

25000/25000 - 2s - loss: 0.0894 - accuracy: 0.9694 - binary_crossentropy: 0.0894 - val_loss: 0.4795 - val_accuracy: 0.8778 - val_binary_crossentropy: 0.4795

Epoch 16/20

25000/25000 - 3s - loss: 0.0832 - accuracy: 0.9718 - binary_crossentropy: 0.0832 - val_loss: 0.4720 - val_accuracy: 0.8755 - val_binary_crossentropy: 0.4720

Epoch 17/20

25000/25000 - 3s - loss: 0.0809 - accuracy: 0.9721 - binary_crossentropy: 0.0809 - val_loss: 0.5038 - val_accuracy: 0.8758 - val_binary_crossentropy: 0.5038

Epoch 18/20

25000/25000 - 2s - loss: 0.0723 - accuracy: 0.9753 - binary_crossentropy: 0.0723 - val_loss: 0.5267 - val_accuracy: 0.8745 - val_binary_crossentropy: 0.5267

Epoch 19/20

25000/25000 - 2s - loss: 0.0680 - accuracy: 0.9758 - binary_crossentropy: 0.0680 - val_loss: 0.5478 - val_accuracy: 0.8750 - val_binary_crossentropy: 0.5478

Epoch 20/20

25000/25000 - 2s - loss: 0.0695 - accuracy: 0.9760 - binary_crossentropy: 0.0695 - val_loss: 0.5537 - val_accuracy: 0.8757 - val_binary_crossentropy: 0.5537
:end:

Epoch 20/20

25000/25000 - 2s - loss: 0.0695 - accuracy: 0.9760 - binary_crossentropy: 0.0695 - val_loss: 0.5537 - val_accuracy: 0.8757 - val_binary_crossentropy: 0.5537

#+NAME: 2d0fcb41-94ea-4dc8-a138-68c27e761bff
#+BEGIN_SRC ein-python :session localhost :results raw drawer
  plot_history([('baseline', baseline_history), ('dropout', dpt_model_history)])
#+END_SRC

#+RESULTS: 2d0fcb41-94ea-4dc8-a138-68c27e761bff
:results:
[[file:ein-images/ob-ein-90b79b4977492ab49399c5b65914102b.png]]
:end:
